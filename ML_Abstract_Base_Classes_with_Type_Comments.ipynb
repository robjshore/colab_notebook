{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robjshore/colab_notebook/blob/main/ML_Abstract_Base_Classes_with_Type_Comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "from typing import Any, Dict, List, Optional, Union, Sequence\n",
        "\n",
        "class BaseMLOperation(abc.ABC):\n",
        "    \"\"\"\n",
        "    A base class for all ML operations, providing a common configuration\n",
        "    initialization and validation pattern.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
        "        \"\"\"\n",
        "        Initializes the operation with an optional configuration dictionary.\n",
        "\n",
        "        Args:\n",
        "            config (Optional[Dict[str, Any]]):\n",
        "                Input Type: A dictionary where keys are configuration parameter names (strings)\n",
        "                            and values can be of any type, depending on the specific needs\n",
        "                            of the concrete operation. If None, an empty dictionary is used.\n",
        "                Purpose: To provide settings and parameters that control the behavior\n",
        "                         of the operation (e.g., model paths, API keys, thresholds).\n",
        "        \"\"\"\n",
        "        self.config: Dict[str, Any] = config if config is not None else {}\n",
        "        # Specific validation logic is deferred to subclasses and typically called\n",
        "        # within their `configure` or `run` methods.\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def configure(self) -> None:\n",
        "        \"\"\"\n",
        "        Configures the operation. This method is intended for setup tasks that\n",
        "        might rely on the initial configuration, such as loading models,\n",
        "        allocating resources, or setting up connections.\n",
        "        Implementations should make this method idempotent if possible.\n",
        "\n",
        "        Input Type: None (implicitly `self`, which carries the `config`).\n",
        "        Output Type: None. This method typically modifies the internal state of\n",
        "                     the object (e.g., loads models into `self.model`).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def validate_config(self) -> None:\n",
        "        \"\"\"\n",
        "        Validates the provided configuration to ensure the operation can run.\n",
        "\n",
        "        Input Type: None (implicitly `self`, which carries the `config`).\n",
        "        Output Type: None. This method is expected to raise an exception\n",
        "                     (e.g., ValueError) if the configuration is invalid.\n",
        "        Raises:\n",
        "            ValueError: If the configuration is invalid or missing required parameters.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# --- Document Ingestion ---\n",
        "class BaseDocumentIngestor(BaseMLOperation):\n",
        "    \"\"\"Abstract base class for document ingestion from various sources.\"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def ingest(self, source: Any, **kwargs) -> Any:\n",
        "        \"\"\"\n",
        "        Ingests content from a given source.\n",
        "\n",
        "        Args:\n",
        "            source (Any):\n",
        "                Input Type: The type is `Any` to allow maximum flexibility. This could be:\n",
        "                            - A string representing a file path (e.g., \"/path/to/doc.pdf\").\n",
        "                            - A string representing a URL (e.g., \"http://example.com/page\").\n",
        "                            - A database connection object or query string.\n",
        "                            - An API client or parameters for an API call.\n",
        "                            - Raw text content itself.\n",
        "                Purpose: Specifies where or what to ingest.\n",
        "            **kwargs: Additional keyword arguments specific to the ingestion method.\n",
        "\n",
        "        Returns:\n",
        "            Any:\n",
        "                Output Type: The type is `Any` to accommodate diverse output formats.\n",
        "                             This could be:\n",
        "                             - A single string containing the entire document content.\n",
        "                             - A list of strings, where each string is a page or section.\n",
        "                             - A list of structured objects (e.g., custom Document objects).\n",
        "                             - A dictionary representing structured data (e.g., from JSON/CSV).\n",
        "                Purpose: To provide the ingested content in a usable format for downstream tasks.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# --- Text Chunking ---\n",
        "class BaseTextChunker(BaseMLOperation):\n",
        "    \"\"\"Abstract base class for dividing text into smaller chunks.\"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def chunk(self, document: Any, **kwargs) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Divides a document into smaller, manageable chunks.\n",
        "\n",
        "        Args:\n",
        "            document (Any):\n",
        "                Input Type: The type is `Any` as it depends on the output of an ingestor.\n",
        "                            This could be:\n",
        "                            - A single large string.\n",
        "                            - A list of strings (e.g., pages of a document).\n",
        "                            - A custom document object.\n",
        "                Purpose: The content that needs to be divided into smaller pieces.\n",
        "            **kwargs: Additional keyword arguments for chunking strategy customization\n",
        "                      (e.g., chunk_size, overlap, delimiters).\n",
        "\n",
        "        Returns:\n",
        "            List[Any]:\n",
        "                Output Type: A list where each element is a chunk. The type of each\n",
        "                             chunk (`Any`) can vary:\n",
        "                             - Typically, a list of strings.\n",
        "                             - Could also be a list of custom Chunk objects containing\n",
        "                               text and metadata.\n",
        "                Purpose: To provide the text broken down into manageable units.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# --- Tokenization ---\n",
        "class BaseTokenizer(BaseMLOperation):\n",
        "    \"\"\"Abstract base class for breaking down text into tokens.\"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def tokenize(self, text_input: Union[str, Sequence[str]], **kwargs) -> Union[List[str], List[List[str]]]:\n",
        "        \"\"\"\n",
        "        Converts text input into a sequence of tokens.\n",
        "\n",
        "        Args:\n",
        "            text_input (Union[str, Sequence[str]]):\n",
        "                Input Type: Can be either:\n",
        "                            - A single string (e.g., one document or chunk).\n",
        "                            - A sequence (e.g., list, tuple) of strings (e.g., multiple chunks).\n",
        "                Purpose: The text data to be broken down into individual tokens.\n",
        "            **kwargs: Additional keyword arguments for tokenizer customization.\n",
        "\n",
        "        Returns:\n",
        "            Union[List[str], List[List[str]]]:\n",
        "                Output Type:\n",
        "                            - If input is a single string: A list of strings, where each\n",
        "                              string is a token (e.g., `['hello', 'world']`).\n",
        "                            - If input is a sequence of strings: A list of lists of strings,\n",
        "                              where each inner list contains tokens for the corresponding\n",
        "                              input string (e.g., `[['hello', 'world'], ['another', 'sentence']]`).\n",
        "                Purpose: To provide a tokenized representation of the input text.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# --- Creation of Embeddings ---\n",
        "class BaseEmbedder(BaseMLOperation):\n",
        "    \"\"\"Abstract base class for converting text or tokens into numerical embeddings.\"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def embed(\n",
        "        self,\n",
        "        data_to_embed: Union[str, Sequence[str], Sequence[Sequence[str]]],\n",
        "        **kwargs\n",
        "    ) -> List[List[float]]:\n",
        "        \"\"\"\n",
        "        Generates numerical vector embeddings for the given input.\n",
        "\n",
        "        Args:\n",
        "            data_to_embed (Union[str, Sequence[str], Sequence[Sequence[str]]]):\n",
        "                Input Type: Can be:\n",
        "                            - A single string (e.g., a sentence or document).\n",
        "                            - A sequence of strings (e.g., multiple sentences, chunks).\n",
        "                            - A sequence of sequences of strings (e.g., pre-tokenized text,\n",
        "                              where each inner sequence is a list of tokens for one item).\n",
        "                Purpose: The textual data to be converted into numerical vectors.\n",
        "            **kwargs: Additional keyword arguments for embedding model customization.\n",
        "\n",
        "        Returns:\n",
        "            List[List[float]]:\n",
        "                Output Type: A list of embedding vectors. Each inner list is a vector\n",
        "                             of floats representing an input item.\n",
        "                             (e.g., `[[0.1, 0.2, ...], [0.3, 0.4, ...]]`).\n",
        "                Purpose: To provide numerical representations of the input data suitable\n",
        "                         for similarity comparisons or as input to ML models.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# --- Weight Assignment / Scoring ---\n",
        "class BaseScorer(BaseMLOperation):\n",
        "    \"\"\"Abstract base class for assigning weights or scores to items.\"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def score(\n",
        "        self,\n",
        "        items: Sequence[Any],\n",
        "        context: Optional[Any] = None,\n",
        "        **kwargs\n",
        "    ) -> List[Union[float, int]]:\n",
        "        \"\"\"\n",
        "        Assigns a numerical score or weight to each item in a sequence.\n",
        "\n",
        "        Args:\n",
        "            items (Sequence[Any]):\n",
        "                Input Type: A sequence (e.g., list) of items to be scored. The type of\n",
        "                            each item (`Any`) can vary:\n",
        "                            - Documents, chunks of text (strings).\n",
        "                            - Tokens (strings).\n",
        "                            - Feature vectors (lists of numbers).\n",
        "                Purpose: The collection of items that need scoring.\n",
        "            context (Optional[Any]):\n",
        "                Input Type: `Any` type to allow flexibility. This could be:\n",
        "                            - A query string for relevance scoring.\n",
        "                            - A global document collection for TF-IDF.\n",
        "                            - A set of rules or a model for heuristic scoring.\n",
        "                Purpose: Provides additional information that might be needed to calculate scores.\n",
        "            **kwargs: Additional keyword arguments for scoring algorithm customization.\n",
        "\n",
        "        Returns:\n",
        "            List[Union[float, int]]:\n",
        "                Output Type: A list of numerical scores (floats or integers),\n",
        "                             corresponding one-to-one with the input `items`.\n",
        "                Purpose: To provide a quantitative measure of importance, relevance, or\n",
        "                         some other characteristic for each item.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# --- Clustering ---\n",
        "class BaseClusterer(BaseMLOperation):\n",
        "    \"\"\"Abstract base class for grouping similar items together.\"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def cluster(self, data_points: Sequence[List[float]], **kwargs) -> List[int]:\n",
        "        \"\"\"\n",
        "        Assigns items to clusters based on their features.\n",
        "\n",
        "        Args:\n",
        "            data_points (Sequence[List[float]]):\n",
        "                Input Type: A sequence (e.g., list) of data points. Each data point is\n",
        "                            a list of floats, typically representing a feature vector\n",
        "                            or an embedding.\n",
        "                            (e.g., `[[0.1, 0.2], [0.9, 0.8], [0.15, 0.22]]`).\n",
        "                Purpose: The numerical data to be grouped into clusters.\n",
        "            **kwargs: Additional keyword arguments for clustering algorithm customization\n",
        "                      (e.g., number of clusters for K-Means, epsilon for DBSCAN).\n",
        "\n",
        "        Returns:\n",
        "            List[int]:\n",
        "                Output Type: A list of integers, where each integer is a cluster label\n",
        "                             assigned to the corresponding input data point.\n",
        "                             (e.g., `[0, 1, 0]`, indicating the first and third points\n",
        "                             are in cluster 0, and the second is in cluster 1).\n",
        "                             A value like -1 might indicate noise for some algorithms.\n",
        "                Purpose: To provide group assignments for the input data.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# --- Anomaly Detection ---\n",
        "class BaseAnomalyDetector(BaseMLOperation):\n",
        "    \"\"\"Abstract base class for identifying items that deviate from the norm.\"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def detect(self, data_points: Sequence[List[float]], **kwargs) -> List[Union[int, bool]]:\n",
        "        \"\"\"\n",
        "        Identifies anomalies within a set of data points.\n",
        "\n",
        "        Args:\n",
        "            data_points (Sequence[List[float]]):\n",
        "                Input Type: A sequence (e.g., list) of data points. Each data point is\n",
        "                            a list of floats, typically a feature vector or embedding.\n",
        "                Purpose: The numerical data to be analyzed for anomalies.\n",
        "            **kwargs: Additional keyword arguments for anomaly detection algorithm customization.\n",
        "\n",
        "        Returns:\n",
        "            List[Union[int, bool]]:\n",
        "                Output Type: A list of indicators, one for each input data point.\n",
        "                             The indicator can be:\n",
        "                             - An integer (e.g., 1 for normal, -1 for anomaly, or 0/1).\n",
        "                             - A boolean (e.g., True for anomaly, False for normal).\n",
        "                Purpose: To flag which data points are considered outliers.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# --- Feature Extraction ---\n",
        "class BaseFeatureExtractor(BaseMLOperation):\n",
        "    \"\"\"Abstract base class for deriving features from raw data.\"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def extract(self, raw_data: Any, **kwargs) -> Any:\n",
        "        \"\"\"\n",
        "        Extracts meaningful features from raw input data.\n",
        "\n",
        "        Args:\n",
        "            raw_data (Any):\n",
        "                Input Type: The type is `Any` due to the wide variety of possible inputs:\n",
        "                            - Text (string or list of strings).\n",
        "                            - Image data.\n",
        "                            - Structured data (e.g., dictionaries, pandas DataFrames).\n",
        "                            - Time series data.\n",
        "                Purpose: The input from which features need to be derived.\n",
        "            **kwargs: Additional keyword arguments for feature extraction customization.\n",
        "\n",
        "        Returns:\n",
        "            Any:\n",
        "                Output Type: The type is `Any` as the structure of extracted features\n",
        "                             can vary greatly:\n",
        "                             - A list or array of numerical features.\n",
        "                             - A dictionary of feature names to values.\n",
        "                             - A sequence of complex objects (e.g., POS tags, named entities).\n",
        "                             - A sparse matrix.\n",
        "                Purpose: To provide a transformed representation of the data that is more\n",
        "                         suitable for machine learning models or analysis.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# --- Data Transformation / Preprocessing ---\n",
        "class BaseDataPreprocessor(BaseMLOperation):\n",
        "    \"\"\"Abstract base class for cleaning, normalizing, or transforming data.\"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def preprocess(self, data: Any, **kwargs) -> Any:\n",
        "        \"\"\"\n",
        "        Applies preprocessing steps to the input data.\n",
        "\n",
        "        Args:\n",
        "            data (Any):\n",
        "                Input Type: The type is `Any` because preprocessing can apply to various\n",
        "                            data types:\n",
        "                            - Text (string, list of strings).\n",
        "                            - Numerical data (lists, arrays).\n",
        "                            - Structured data (dictionaries, DataFrames).\n",
        "                Purpose: The data that needs cleaning, normalization, or transformation.\n",
        "            **kwargs: Additional keyword arguments for preprocessing step customization\n",
        "                      (e.g., stopwords list, stemming algorithm, normalization range).\n",
        "\n",
        "        Returns:\n",
        "            Any:\n",
        "                Output Type: The type is `Any`, and it typically matches or is closely\n",
        "                             related to the input data type, but in its processed form.\n",
        "                             (e.g., input string -> output string after lowercasing).\n",
        "                Purpose: To provide data that is cleaner, more consistent, or in a more\n",
        "                         appropriate format for subsequent operations.\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "HuC8YuLImxIx"
      }
    },
    {
      "source": [
        "import os\n",
        "from typing import List, Dict, Any, Optional\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Assuming BaseMLOperation and BaseDocumentIngestor are defined above this code block\n",
        "# as in the user's provided context.\n",
        "# from your_module import BaseMLOperation, BaseDocumentIngestor\n",
        "\n",
        "## Custom Object to Hold Ingested Directory Data\n",
        "\n",
        "class IngestedDirectory:\n",
        "    \"\"\"\n",
        "    A custom object to hold information about files ingested from a directory.\n",
        "    \"\"\"\n",
        "    def __init__(self, directory_path: str, file_paths: List[str]):\n",
        "        \"\"\"\n",
        "        Initializes the IngestedDirectory object.\n",
        "\n",
        "        Args:\n",
        "            directory_path (str): The path to the directory that was ingested.\n",
        "            file_paths (List[str]): A list of full paths to the files found in the directory.\n",
        "        \"\"\"\n",
        "        self.directory_path = directory_path\n",
        "        self.file_paths = file_paths\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"String representation for easy printing.\"\"\"\n",
        "        return f\"IngestedDirectory(path='{self.directory_path}', files={len(self.file_paths)})\"\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Representation for debugging.\"\"\"\n",
        "        return f\"IngestedDirectory(directory_path='{self.directory_path}', file_paths={self.file_paths})\"\n",
        "\n",
        "    # You could add methods here to facilitate access for a chunker,\n",
        "    # e.g., get_file_contents(self, file_path) -> str or get_all_contents(self) -> Dict[str, str]\n",
        "    # but for this example, the chunker can access the self.file_paths list directly.\n",
        "\n",
        "## Directory Ingestor Class\n",
        "\n",
        "class DirectoryIngestor(BaseDocumentIngestor):\n",
        "    \"\"\"\n",
        "    Ingests a directory by listing its contents.\n",
        "    Returns an IngestedDirectory object containing file paths.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
        "        \"\"\"\n",
        "        Initializes the DirectoryIngestor with configuration.\n",
        "\n",
        "        Config should contain:\n",
        "        - 'directory_path' (str): The path to the directory to ingest.\n",
        "        \"\"\"\n",
        "        super().__init__(config)\n",
        "        self.directory_path: Optional[str] = None\n",
        "\n",
        "    def validate_config(self) -> None:\n",
        "        \"\"\"\n",
        "        Validates the configuration for the DirectoryIngestor.\n",
        "\n",
        "        Checks if 'directory_path' is provided and is a valid directory.\n",
        "        \"\"\"\n",
        "        if 'directory_path' not in self.config or not isinstance(self.config['directory_path'], str):\n",
        "            raise ValueError(\"Configuration must contain a 'directory_path' key with a string value.\")\n",
        "\n",
        "        self.directory_path = self.config['directory_path']\n",
        "\n",
        "        if not os.path.isdir(self.directory_path):\n",
        "             raise ValueError(f\"The provided directory_path '{self.directory_path}' is not a valid directory.\")\n",
        "\n",
        "    def configure(self) -> None:\n",
        "        \"\"\"\n",
        "        Configures the DirectoryIngestor.\n",
        "        Validation is done here based on the BaseMLOperation pattern.\n",
        "        \"\"\"\n",
        "        # In this simple case, configure mainly involves validating the config\n",
        "        # and storing the validated directory path.\n",
        "        self.validate_config()\n",
        "\n",
        "\n",
        "    def ingest(self, source: Any = None, **kwargs) -> IngestedDirectory:\n",
        "        \"\"\"\n",
        "        Lists files in the configured directory.\n",
        "\n",
        "        Args:\n",
        "            source (Any): Ignored in this implementation as the directory is\n",
        "                          specified in the config.\n",
        "            **kwargs: Additional keyword arguments (ignored).\n",
        "\n",
        "        Returns:\n",
        "            IngestedDirectory: An object containing the directory path and a list\n",
        "                               of file paths within that directory.\n",
        "        \"\"\"\n",
        "        if self.directory_path is None:\n",
        "             # Ensure configuration and validation has happened\n",
        "             self.configure()\n",
        "\n",
        "        file_list = []\n",
        "        # Walk through the directory to get all file paths\n",
        "        for root, _, files in os.walk(self.directory_path):\n",
        "            for file in files:\n",
        "                file_list.append(os.path.join(root, file))\n",
        "\n",
        "        return IngestedDirectory(directory_path=self.directory_path, file_paths=file_list)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RRcMBHaIvCtO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "source": [
        "import abc\n",
        "from typing import Any, Dict, List, Optional, Union, Sequence\n",
        "import os\n",
        "\n",
        "# Assuming BaseMLOperation and BaseTextChunker are defined from the previous code block\n",
        "# from your_module import BaseMLOperation, BaseTextChunker\n",
        "\n",
        "## Custom Object to Hold Ingested Directory Data (from previous code)\n",
        "class IngestedDirectory:\n",
        "    \"\"\"\n",
        "    A custom object to hold information about files ingested from a directory.\n",
        "    \"\"\"\n",
        "    def __init__(self, directory_path: str, file_paths: List[str]):\n",
        "        self.directory_path = directory_path\n",
        "        self.file_paths = file_paths\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"IngestedDirectory(path='{self.directory_path}', files={len(self.file_paths)})\"\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"IngestedDirectory(directory_path='{self.directory_path}', file_paths={self.file_paths})\"\n",
        "\n",
        "    # Methods to access contents would go here in a real implementation\n",
        "    # For demonstration, a chunker will access self.file_paths directly.\n",
        "\n",
        "\n",
        "## A Dummy Chunker Class for Demonstration\n",
        "class DummyDirectoryChunker(BaseTextChunker):\n",
        "    \"\"\"\n",
        "    A dummy chunker that accepts an IngestedDirectory object.\n",
        "    This class is for demonstration purposes and does not perform actual chunking.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
        "        \"\"\"Initializes the dummy chunker.\"\"\"\n",
        "        super().__init__(config)\n",
        "\n",
        "    def validate_config(self) -> None:\n",
        "        \"\"\"\n",
        "        Dummy config validation.\n",
        "        In a real chunker, this would validate chunking parameters.\n",
        "        \"\"\"\n",
        "        print(\"DummyDirectoryChunker: Validating config (no-op).\")\n",
        "        # Add real config validation here if needed for the dummy\n",
        "\n",
        "    def configure(self) -> None:\n",
        "        \"\"\"\n",
        "        Dummy configuration step.\n",
        "        In a real chunker, this might load resources or set up parameters.\n",
        "        \"\"\"\n",
        "        print(\"DummyDirectoryChunker: Configuring (no-op).\")\n",
        "        self.validate_config() # Call validation as per BaseMLOperation pattern\n",
        "\n",
        "    def chunk(self, document: Any, **kwargs) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Dummy chunk method that expects an IngestedDirectory object.\n",
        "        Does not perform actual chunking.\n",
        "\n",
        "        Args:\n",
        "            document (Any): Expected to be an instance of IngestedDirectory.\n",
        "            **kwargs: Additional keyword arguments (ignored).\n",
        "\n",
        "        Returns:\n",
        "            List[Any]: An empty list as this is a dummy implementation.\n",
        "        \"\"\"\n",
        "        print(\"DummyDirectoryChunker: Received document for chunking.\")\n",
        "\n",
        "        if not isinstance(document, IngestedDirectory):\n",
        "            print(f\"Warning: Expected IngestedDirectory object but received {type(document)}\")\n",
        "            return []\n",
        "\n",
        "        print(f\"DummyDirectoryChunker: Accessing ingested directory: {document.directory_path}\")\n",
        "        print(f\"DummyDirectoryChunker: Found {len(document.file_paths)} files to process.\")\n",
        "\n",
        "        # In a real chunker, you would iterate through document.file_paths,\n",
        "        # read the content of each file, and then apply chunking logic.\n",
        "        # For example:\n",
        "        # all_content = \"\"\n",
        "        # for file_path in document.file_paths:\n",
        "        #     try:\n",
        "        #         with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        #             all_content += f.read() + \"\\n\\n\" # Concatenate content, maybe add separators\n",
        "        #     except Exception as e:\n",
        "        #         print(f\"Error reading file {file_path}: {e}\")\n",
        "        #\n",
        "        # # Now apply your chunking logic to all_content or process files individually\n",
        "        # actual_chunks = [\"dummy chunk 1\", \"dummy chunk 2\"] # Replace with real chunking result\n",
        "\n",
        "        print(\"DummyDirectoryChunker: (Skipping actual chunking in dummy class)\")\n",
        "\n",
        "        # Return a placeholder or empty list\n",
        "        return [] # In a real scenario, return the list of chunks\n",
        "\n",
        "# --- Demonstration Code ---\n",
        "\n",
        "## Create a dummy IngestedDirectory object\n",
        "# For this demonstration, let's create a temporary dummy directory and file\n",
        "dummy_dir = \"./dummy_ingest_dir\"\n",
        "dummy_file1 = os.path.join(dummy_dir, \"file1.txt\")\n",
        "dummy_file2 = os.path.join(dummy_dir, \"subdir\", \"file2.txt\")\n",
        "\n",
        "# Create the dummy directory and a subdirectory\n",
        "os.makedirs(os.path.join(dummy_dir, \"subdir\"), exist_ok=True)\n",
        "\n",
        "# Create dummy files\n",
        "with open(dummy_file1, \"w\") as f:\n",
        "    f.write(\"This is the content of dummy file 1.\")\n",
        "with open(dummy_file2, \"w\") as f:\n",
        "    f.write(\"This is the content of dummy file 2 in a subdirectory.\")\n",
        "\n",
        "# Manually create the list of dummy file paths\n",
        "dummy_file_paths = [dummy_file1, dummy_file2]\n",
        "\n",
        "# Create an instance of the custom IngestedDirectory object\n",
        "# In a real pipeline, this object would come from the DirectoryIngestor.ingest() call\n",
        "print(\"Creating a dummy IngestedDirectory object...\")\n",
        "ingested_directory_object = IngestedDirectory(directory_path=dummy_dir, file_paths=dummy_file_paths)\n",
        "print(f\"Created object: {ingested_directory_object}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "## Create an instance of the Dummy Chunker class\n",
        "print(\"Creating an instance of DummyDirectoryChunker...\")\n",
        "chunker_instance = DummyDirectoryChunker()\n",
        "print(\"Chunker instance created.\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "## Pass the IngestedDirectory object to the chunker's chunk method\n",
        "print(\"Passing the IngestedDirectory object to the chunker's chunk method...\")\n",
        "try:\n",
        "    # The chunk method will be called with the ingested object\n",
        "    resulting_chunks = chunker_instance.chunk(ingested_directory_object)\n",
        "    print(\"Chunking method called.\")\n",
        "    print(f\"Resulting chunks (from dummy): {resulting_chunks}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during dummy chunking: {e}\")\n",
        "\n",
        "print(\"-\" * 20)\n",
        "print(\"Demonstration complete.\")\n",
        "\n",
        "# Clean up the dummy directory and files\n",
        "print(\"Cleaning up dummy directory...\")\n",
        "os.remove(dummy_file1)\n",
        "os.remove(dummy_file2)\n",
        "os.rmdir(os.path.join(dummy_dir, \"subdir\"))\n",
        "os.rmdir(dummy_dir)\n",
        "print(\"Cleanup complete.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elMCaK7VvVDP",
        "outputId": "dd04d548-eb1c-4700-966a-da8bb4226473"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a dummy IngestedDirectory object...\n",
            "Created object: IngestedDirectory(path='./dummy_ingest_dir', files=2)\n",
            "--------------------\n",
            "Creating an instance of DummyDirectoryChunker...\n",
            "Chunker instance created.\n",
            "--------------------\n",
            "Passing the IngestedDirectory object to the chunker's chunk method...\n",
            "DummyDirectoryChunker: Received document for chunking.\n",
            "DummyDirectoryChunker: Accessing ingested directory: ./dummy_ingest_dir\n",
            "DummyDirectoryChunker: Found 2 files to process.\n",
            "DummyDirectoryChunker: (Skipping actual chunking in dummy class)\n",
            "Chunking method called.\n",
            "Resulting chunks (from dummy): []\n",
            "--------------------\n",
            "Demonstration complete.\n",
            "Cleaning up dummy directory...\n",
            "Cleanup complete.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import abc\n",
        "from typing import Any, Dict, List, Optional, Union, Sequence\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Assuming BaseMLOperation and BaseTextChunker are defined\n",
        "# from your_module import BaseMLOperation, BaseTextChunker\n",
        "\n",
        "# Import necessary libraries from transformers and torch\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    import torch\n",
        "except ImportError:\n",
        "    print(\"Please install transformers and torch: pip install transformers torch\")\n",
        "    # Exit or handle the error appropriately\n",
        "    AutoTokenizer = None\n",
        "    AutoModel = None\n",
        "    torch = None\n",
        "\n",
        "# Import sentence tokenizer from NLTK\n",
        "try:\n",
        "    import nltk\n",
        "    # Download the punkt tokenizer if you haven't already\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except nltk.downloader.DownloadError:\n",
        "        nltk.download('punkt')\n",
        "    from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
        "except ImportError:\n",
        "    print(\"Please install nltk: pip install nltk\")\n",
        "    # Exit or handle the error appropriately\n",
        "    sent_tokenize = None\n",
        "    PunktSentenceTokenizer = None\n",
        "\n",
        "\n",
        "## LegalBERT Semantic Chunker\n",
        "\n",
        "class LegalBertSemanticChunker(BaseTextChunker):\n",
        "    \"\"\"\n",
        "    A chunker that uses a LegalBERT model to find potential semantic boundaries\n",
        "    in text and outputs offsets to a JSON file.\n",
        "\n",
        "    This implementation uses sentence boundaries as a proxy for semantic boundaries\n",
        "    and calculates their end offsets. A more advanced implementation might\n",
        "    process text in windows and analyze model outputs (like embeddings or attention)\n",
        "    to detect changes in topic or meaning.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
        "        \"\"\"\n",
        "        Initializes the LegalBertSemanticChunker.\n",
        "\n",
        "        Config can optionally include:\n",
        "        - 'model_name' (str): The Hugging Face model name (default: 'legalbert-base-uncased').\n",
        "        - 'output_json_path' (str): The path to save the output JSON file. Required.\n",
        "        - 'device' (str): 'cuda' or 'cpu' (default: 'cpu').\n",
        "        \"\"\"\n",
        "        super().__init__(config)\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.output_json_path: Optional[str] = None\n",
        "        self.model_name: str = self.config.get('model_name', 'legalbert-base-uncased')\n",
        "        self.device: str = self.config.get('device', 'cpu')\n",
        "\n",
        "    def validate_config(self) -> None:\n",
        "        \"\"\"\n",
        "        Validates the configuration.\n",
        "        Checks for 'output_json_path'.\n",
        "        \"\"\"\n",
        "        if 'output_json_path' not in self.config or not isinstance(self.config['output_json_path'], str):\n",
        "            raise ValueError(\"Configuration must contain an 'output_json_path' key with a string value.\")\n",
        "        self.output_json_path = self.config['output_json_path']\n",
        "\n",
        "        # Check if model name and device are valid (basic check)\n",
        "        if not isinstance(self.model_name, str) or len(self.model_name) == 0:\n",
        "             raise ValueError(\"Config 'model_name' must be a non-empty string.\")\n",
        "        if self.device not in ['cpu', 'cuda']:\n",
        "             raise ValueError(\"Config 'device' must be 'cpu' or 'cuda'.\")\n",
        "\n",
        "        print(\"LegalBertSemanticChunker: Config validated successfully.\")\n",
        "\n",
        "    def configure(self) -> None:\n",
        "        \"\"\"\n",
        "        Configures the chunker by loading the model and tokenizer.\n",
        "        \"\"\"\n",
        "        self.validate_config()\n",
        "\n",
        "        if AutoTokenizer is None or AutoModel is None or torch is None or sent_tokenize is None:\n",
        "             raise RuntimeError(\"Required libraries (transformers, torch, nltk) not loaded. Please install them.\")\n",
        "\n",
        "        print(f\"LegalBertSemanticChunker: Loading model and tokenizer: {self.model_name}\")\n",
        "        try:\n",
        "            # Load tokenizer and model\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            self.model = AutoModel.from_pretrained(self.model_name)\n",
        "            self.model.to(self.device) # Move model to the specified device\n",
        "            self.model.eval() # Set model to evaluation mode\n",
        "            print(\"LegalBertSemanticChunker: Model and tokenizer loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model or tokenizer '{self.model_name}': {e}\")\n",
        "\n",
        "    def chunk(self, document: Any, **kwargs) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Finds semantic boundaries (sentence endings) in the input text\n",
        "        and saves their offsets to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            document (Any): The input text content, expected as a single string.\n",
        "                            If the input is from IngestedDirectory, you need to\n",
        "                            read file content first.\n",
        "            **kwargs: Additional keyword arguments (ignored).\n",
        "\n",
        "        Returns:\n",
        "            List[Any]: In this implementation, returns an empty list as the primary\n",
        "                       output is the JSON file. In a real chunker, this would\n",
        "                       return the text chunks themselves.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None or self.model is None:\n",
        "            # Configure if not already done\n",
        "            self.configure()\n",
        "\n",
        "        if not isinstance(document, str):\n",
        "             print(f\"Warning: LegalBertSemanticChunker expects string input, but received {type(document)}. Attempting to convert.\")\n",
        "             try:\n",
        "                 text = str(document)\n",
        "             except Exception:\n",
        "                 print(\"Error: Could not convert input document to string.\")\n",
        "                 return [] # Return empty list if input is not a string\n",
        "\n",
        "        else:\n",
        "            text = document\n",
        "\n",
        "        if not text.strip():\n",
        "            print(\"Warning: Input text is empty or only whitespace. Skipping chunking.\")\n",
        "            return []\n",
        "\n",
        "        print(f\"LegalBertSemanticChunker: Processing text (length: {len(text)})...\")\n",
        "\n",
        "        # Use NLTK's sentence tokenizer to find potential boundaries\n",
        "        # This is a simplification; a more advanced method would use LegalBERT's\n",
        "        # properties to find boundaries directly or analyze embeddings.\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Calculate the end offsets of each sentence in the original text\n",
        "        offsets: List[int] = []\n",
        "        current_offset = 0\n",
        "        for sentence in sentences:\n",
        "            # Find the position of the sentence within the remaining text\n",
        "            # We add 1 for the space/newline that might separate sentences,\n",
        "            # though this can be tricky with inconsistent spacing.\n",
        "            # A more robust approach might use string finding with original text and track indices.\n",
        "            try:\n",
        "                 # Find the *exact* end position of the sentence in the original text\n",
        "                 # This requires careful index tracking\n",
        "                 end_pos = text.find(sentence, current_offset) + len(sentence)\n",
        "                 if text.find(sentence, current_offset) != -1: # Ensure sentence was found\n",
        "                     offsets.append(end_pos)\n",
        "                     current_offset = end_pos # Update offset for the next search\n",
        "                 else:\n",
        "                      print(f\"Warning: Could not find sentence '{sentence[:50]}...' at expected offset {current_offset}. Offset calculation may be inaccurate.\")\n",
        "                      # Fallback: just add length, less accurate\n",
        "                      current_offset += len(sentence)\n",
        "                      offsets.append(current_offset)\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"Error calculating offset for sentence '{sentence[:50]}...': {e}\")\n",
        "                 # Attempt to recover by just advancing the offset\n",
        "                 current_offset += len(sentence)\n",
        "\n",
        "\n",
        "        print(f\"LegalBertSemanticChunker: Found {len(offsets)} potential semantic boundaries.\")\n",
        "\n",
        "        # Save the offsets to a JSON file\n",
        "        try:\n",
        "            with open(self.output_json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(offsets, f, indent=4)\n",
        "            print(f\"LegalBertSemanticChunker: Offsets saved to {self.output_json_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving offsets to JSON file {self.output_json_path}: {e}\")\n",
        "\n",
        "        # According to the BaseTextChunker definition, chunk() should return the chunks.\n",
        "        # However, in this specific request, the primary output is the JSON file of offsets.\n",
        "        # We will return an empty list as a placeholder or could potentially return\n",
        "        # the text segments based on these offsets if needed for a downstream task.\n",
        "        # For this specific request's output format (JSON offsets), an empty list is acceptable\n",
        "        # as the main result is the side effect of writing the file.\n",
        "        return [] # Returning an empty list of chunks\n",
        "\n",
        "\n",
        "# --- Demonstration Code ---\n",
        "\n",
        "# Define a dummy text input (replace with reading a file if needed)\n",
        "dummy_text_content = \"\"\"\n",
        "This is the first sentence about contracts. It discusses section 1.\n",
        "A second sentence about legal proceedings follows. This sentence is longer and might contain more complex terms.\n",
        "Finally, a third sentence concludes this short document. It talks about compliance requirements.\n",
        "\"\"\"\n",
        "\n",
        "# Define where to save the output JSON file\n",
        "output_json_file = \"./legalbert_offsets.json\"\n",
        "\n",
        "# Create an instance of the LegalBertSemanticChunker\n",
        "# Provide the output JSON path and optionally model name and device\n",
        "print(\"Creating an instance of LegalBertSemanticChunker...\")\n",
        "try:\n",
        "    chunker_config = {\n",
        "        'output_json_path': output_json_file,\n",
        "        'model_name': 'legalbert-base-uncased', # Or another compatible model\n",
        "        'device': 'cuda' if torch.cuda.is_available() else 'cpu' # Use GPU if available\n",
        "    }\n",
        "    legalbert_chunker = LegalBertSemanticChunker(config=chunker_config)\n",
        "    print(\"Chunker instance created.\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Configure the chunker (loads the model and tokenizer)\n",
        "    # This is called automatically by chunk if not called explicitly\n",
        "    # print(\"Configuring the chunker (loading model)...\")\n",
        "    # legalbert_chunker.configure()\n",
        "    # print(\"Chunker configured.\")\n",
        "    # print(\"-\" * 20)\n",
        "\n",
        "    # Pass the text content to the chunk method\n",
        "    print(\"Passing the text content to the chunker's chunk method...\")\n",
        "    resulting_chunks = legalbert_chunker.chunk(dummy_text_content)\n",
        "    print(\"Chunking method called.\")\n",
        "    print(f\"Resulting chunks (this chunker outputs offsets to JSON): {resulting_chunks}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Verify the output file was created\n",
        "    if os.path.exists(output_json_file):\n",
        "        print(f\"Output JSON file created at: {output_json_file}\")\n",
        "        with open(output_json_file, 'r', encoding='utf-8') as f:\n",
        "             loaded_offsets = json.load(f)\n",
        "        print(f\"Offsets written to JSON: {loaded_offsets}\")\n",
        "\n",
        "        # Clean up the dummy output file\n",
        "        print(\"Cleaning up dummy output file...\")\n",
        "        os.remove(output_json_file)\n",
        "        print(\"Cleanup complete.\")\n",
        "    else:\n",
        "        print(f\"Output JSON file was NOT created at: {output_json_file}\")\n",
        "\n",
        "\n",
        "except RuntimeError as e:\n",
        "     print(f\"Setup Error: {e}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Configuration Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NRyuEIBvw5of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Install required libraries\n",
        "print(\"Installing required libraries...\")\n",
        "!pip install transformers torch nltk\n",
        "print(\"Installation complete.\")\n",
        "\n",
        "# Download NLTK data (punkt tokenizer)\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except (nltk.downloader.DownloadError, LookupError):\n",
        "    print(\"Downloading NLTK punkt tokenizer...\")\n",
        "    nltk.download('punkt')\n",
        "    print(\"Download complete.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XZm8KpusxL59",
        "outputId": "af418e40-f179-4a6a-d930-d531130eaa12"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries...\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m874.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m748.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Installation complete.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'nltk.downloader' has no attribute 'DownloadError'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6ab65fdc8fcf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6ab65fdc8fcf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading NLTK punkt tokenizer...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.downloader' has no attribute 'DownloadError'"
          ]
        }
      ]
    },
    {
      "source": [
        "# file ipython-input-0-6ab65fdc8fcf\n",
        "import abc\n",
        "from typing import Any, Dict, List, Optional, Union, Sequence\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Assuming BaseMLOperation and BaseTextChunker are defined\n",
        "# from your_module import BaseMLOperation, BaseTextChunker\n",
        "\n",
        "# Import necessary libraries from transformers and torch\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    import torch\n",
        "except ImportError:\n",
        "    print(\"Please install transformers and torch: pip install transformers torch\")\n",
        "    # Exit or handle the error appropriately\n",
        "    AutoTokenizer = None\n",
        "    AutoModel = None\n",
        "    torch = None\n",
        "\n",
        "# Import sentence tokenizer from NLTK\n",
        "try:\n",
        "    import nltk\n",
        "    # Download the punkt tokenizer if you haven't already\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError: # Catch only LookupError here\n",
        "        print(\"NLTK 'punkt' tokenizer not found. Downloading...\")\n",
        "        nltk.download('punkt')\n",
        "        print(\"NLTK 'punkt' tokenizer download complete.\")\n",
        "    from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
        "except ImportError:\n",
        "    print(\"Please install nltk: pip install nltk\")\n",
        "    # Exit or handle the error appropriately\n",
        "    sent_tokenize = None\n",
        "    PunktSentenceTokenizer = None\n",
        "\n",
        "\n",
        "## LegalBERT Semantic Chunker\n",
        "\n",
        "class LegalBertSemanticChunker(BaseTextChunker):\n",
        "    \"\"\"\n",
        "    A chunker that uses a LegalBERT model to find potential semantic boundaries\n",
        "    in text and outputs offsets to a JSON file.\n",
        "\n",
        "    This implementation uses sentence boundaries as a proxy for semantic boundaries\n",
        "    and calculates their end offsets. A more advanced implementation might\n",
        "    process text in windows and analyze model outputs (like embeddings or attention)\n",
        "    to detect changes in topic or meaning.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
        "        \"\"\"\n",
        "        Initializes the LegalBertSemanticChunker.\n",
        "\n",
        "        Config can optionally include:\n",
        "        - 'model_name' (str): The Hugging Face model name (default: 'legalbert-base-uncased').\n",
        "        - 'output_json_path' (str): The path to save the output JSON file. Required.\n",
        "        - 'device' (str): 'cuda' or 'cpu' (default: 'cpu').\n",
        "        \"\"\"\n",
        "        super().__init__(config)\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.output_json_path: Optional[str] = None\n",
        "        # CORRECTED: Use the correct model name from Hugging Face Hub\n",
        "        self.model_name: str = self.config.get('model_name', 'nlpaueb/legal-bert-base-uncased')\n",
        "        self.device: str = self.config.get('device', 'cpu')\n",
        "\n",
        "    def validate_config(self) -> None:\n",
        "        \"\"\"\n",
        "        Validates the configuration.\n",
        "        Checks for 'output_json_path'.\n",
        "        \"\"\"\n",
        "        if 'output_json_path' not in self.config or not isinstance(self.config['output_json_path'], str):\n",
        "            raise ValueError(\"Configuration must contain an 'output_json_path' key with a string value.\")\n",
        "        self.output_json_path = self.config['output_json_path']\n",
        "\n",
        "        # Check if model name and device are valid (basic check)\n",
        "        if not isinstance(self.model_name, str) or len(self.model_name) == 0:\n",
        "             raise ValueError(\"Config 'model_name' must be a non-empty string.\")\n",
        "        if self.device not in ['cpu', 'cuda']:\n",
        "             raise ValueError(\"Config 'device' must be 'cpu' or 'cuda'.\")\n",
        "\n",
        "        print(\"LegalBertSemanticChunker: Config validated successfully.\")\n",
        "\n",
        "    def configure(self) -> None:\n",
        "        \"\"\"\n",
        "        Configures the chunker by loading the model and tokenizer.\n",
        "        \"\"\"\n",
        "        self.validate_config()\n",
        "\n",
        "        if AutoTokenizer is None or AutoModel is None or torch is None or sent_tokenize is None:\n",
        "             raise RuntimeError(\"Required libraries (transformers, torch, nltk) not loaded. Please install them.\")\n",
        "\n",
        "        print(f\"LegalBertSemanticChunker: Loading model and tokenizer: {self.model_name}\")\n",
        "        try:\n",
        "            # Load tokenizer and model\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            self.model = AutoModel.from_pretrained(self.model_name)\n",
        "            self.model.to(self.device) # Move model to the specified device\n",
        "            self.model.eval() # Set model to evaluation mode\n",
        "            print(\"LegalBertSemanticChunker: Model and tokenizer loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model or tokenizer '{self.model_name}': {e}\")\n",
        "\n",
        "    def chunk(self, document: Any, **kwargs) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Finds semantic boundaries (sentence endings) in the input text\n",
        "        and saves their offsets to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            document (Any): The input text content, expected as a single string.\n",
        "                            If the input is from IngestedDirectory, you need to\n",
        "                            read file content first.\n",
        "            **kwargs: Additional keyword arguments (ignored).\n",
        "\n",
        "        Returns:\n",
        "            List[Any]: In this implementation, returns an empty list as the primary\n",
        "                       output is the JSON file. In a real chunker, this would\n",
        "                       return the text chunks themselves.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None or self.model is None:\n",
        "            # Configure if not already done\n",
        "            self.configure()\n",
        "\n",
        "        if not isinstance(document, str):\n",
        "             print(f\"Warning: LegalBertSemanticChunker expects string input, but received {type(document)}. Attempting to convert.\")\n",
        "             try:\n",
        "                 text = str(document)\n",
        "             except Exception:\n",
        "                 print(\"Error: Could not convert input document to string.\")\n",
        "                 return [] # Return empty list if input is not a string\n",
        "\n",
        "        else:\n",
        "            text = document\n",
        "\n",
        "        if not text.strip():\n",
        "            print(\"Warning: Input text is empty or only whitespace. Skipping chunking.\")\n",
        "            return []\n",
        "\n",
        "        print(f\"LegalBertSemanticChunker: Processing text (length: {len(text)})...\")\n",
        "\n",
        "        # Use NLTK's sentence tokenizer to find potential boundaries\n",
        "        # This is a simplification; a more advanced method would use LegalBERT's\n",
        "        # properties to find boundaries directly or analyze embeddings.\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Calculate the end offsets of each sentence in the original text\n",
        "        offsets: List[int] = []\n",
        "        current_offset = 0\n",
        "        for sentence in sentences:\n",
        "            # Find the position of the sentence within the remaining text\n",
        "            # We add 1 for the space/newline that might separate sentences,\n",
        "            # though this can be tricky with inconsistent spacing.\n",
        "            # A more robust approach might use string finding with original text and track indices.\n",
        "            try:\n",
        "                 # Find the *exact* end position of the sentence in the original text\n",
        "                 # This requires careful index tracking\n",
        "                 end_pos = text.find(sentence, current_offset) + len(sentence)\n",
        "                 if text.find(sentence, current_offset) != -1: # Ensure sentence was found\n",
        "                     offsets.append(end_pos)\n",
        "                     current_offset = end_pos # Update offset for the next search\n",
        "                 else:\n",
        "                      print(f\"Warning: Could not find sentence '{sentence[:50]}...' at expected offset {current_offset}. Offset calculation may be inaccurate.\")\n",
        "                      # Fallback: just add length, less accurate\n",
        "                      current_offset += len(sentence)\n",
        "                      offsets.append(current_offset)\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"Error calculating offset for sentence '{sentence[:50]}...': {e}\")\n",
        "                 # Attempt to recover by just advancing the offset\n",
        "                 current_offset += len(sentence)\n",
        "\n",
        "\n",
        "        print(f\"LegalBertSemanticChunker: Found {len(offsets)} potential semantic boundaries.\")\n",
        "\n",
        "        # Save the offsets to a JSON file\n",
        "        try:\n",
        "            with open(self.output_json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(offsets, f, indent=4)\n",
        "            print(f\"LegalBertSemanticChunker: Offsets saved to {self.output_json_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving offsets to JSON file {self.output_json_path}: {e}\")\n",
        "\n",
        "        # According to the BaseTextChunker definition, chunk() should return the chunks.\n",
        "        # However, in this specific request, the primary output is the JSON file of offsets.\n",
        "        # We will return an empty list as a placeholder or could potentially return\n",
        "        # the text segments based on these offsets if needed for a downstream task.\n",
        "        # For this specific request's output format (JSON offsets), an empty list is acceptable\n",
        "        # as the main result is the side effect of writing the file.\n",
        "        return [] # Returning an empty list of chunks\n",
        "\n",
        "\n",
        "# --- Demonstration Code ---\n",
        "\n",
        "# Define a dummy text input (replace with reading a file if needed)\n",
        "dummy_text_content = \"\"\"\n",
        "This is the first sentence about contracts. It discusses section 1.\n",
        "A second sentence about legal proceedings follows. This sentence is longer and might contain more complex terms.\n",
        "Finally, a third sentence concludes this short document. It talks about compliance requirements.\n",
        "\"\"\"\n",
        "\n",
        "# Define where to save the output JSON file\n",
        "output_json_file = \"./legalbert_offsets.json\"\n",
        "\n",
        "# Create an instance of the LegalBertSemanticChunker\n",
        "# Provide the output JSON path and optionally model name and device\n",
        "print(\"Creating an instance of LegalBertSemanticChunker...\")\n",
        "try:\n",
        "    chunker_config = {\n",
        "        'output_json_path': output_json_file,\n",
        "        # CORRECTED model name\n",
        "        'model_name': 'nlpaueb/legal-bert-base-uncased', # Or another compatible model\n",
        "        'device': 'cuda' if torch.cuda.is_available() else 'cpu' # Use GPU if available\n",
        "    }\n",
        "    legalbert_chunker = LegalBertSemanticChunker(config=chunker_config)\n",
        "    print(\"Chunker instance created.\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Configure the chunker (loads the model and tokenizer)\n",
        "    # This is called automatically by chunk if not called explicitly\n",
        "    # print(\"Configuring the chunker (loading model)...\")\n",
        "    # legalbert_chunker.configure()\n",
        "    # print(\"Chunker configured.\")\n",
        "    # print(\"-\" * 20)\n",
        "\n",
        "    # Pass the text content to the chunk method\n",
        "    print(\"Passing the text content to the chunker's chunk method...\")\n",
        "    resulting_chunks = legalbert_chunker.chunk(dummy_text_content)\n",
        "    print(\"Chunking method called.\")\n",
        "    print(f\"Resulting chunks (this chunker outputs offsets to JSON): {resulting_chunks}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Verify the output file was created\n",
        "    if os.path.exists(output_json_file):\n",
        "        print(f\"Output JSON file created at: {output_json_file}\")\n",
        "        with open(output_json_file, 'r', encoding='utf-8') as f:\n",
        "             loaded_offsets = json.load(f)\n",
        "        print(f\"Offsets written to JSON: {loaded_offsets}\")\n",
        "\n",
        "        # Clean up the dummy output file\n",
        "        print(\"Cleaning up dummy output file...\")\n",
        "        os.remove(output_json_file)\n",
        "        print(\"Cleanup complete.\")\n",
        "    else:\n",
        "        print(f\"Output JSON file was NOT created at: {output_json_file}\")\n",
        "\n",
        "\n",
        "except RuntimeError as e:\n",
        "     print(f\"Setup Error: {e}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Configuration Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDwBTaWfywDE",
        "outputId": "605c30ec-c72e-482b-8f51-d716d5177a19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating an instance of LegalBertSemanticChunker...\n",
            "Chunker instance created.\n",
            "--------------------\n",
            "Passing the text content to the chunker's chunk method...\n",
            "LegalBertSemanticChunker: Config validated successfully.\n",
            "LegalBertSemanticChunker: Loading model and tokenizer: nlpaueb/legal-bert-base-uncased\n",
            "LegalBertSemanticChunker: Model and tokenizer loaded successfully.\n",
            "LegalBertSemanticChunker: Processing text (length: 279)...\n",
            "An unexpected error occurred: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "\n",
        "try:\n",
        "    # Check if the 'punkt_tab' resource is already available\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "    print(\"'punkt_tab' resource is already downloaded.\")\n",
        "except LookupError:\n",
        "    # If 'punkt_tab' is not found, download it\n",
        "    print(\"Downloading NLTK 'punkt_tab' resource...\")\n",
        "    try:\n",
        "        nltk.download('punkt_tab')\n",
        "        print(\"'punkt_tab' resource download complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading 'punkt_tab' resource: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while checking for 'punkt_tab': {e}\")\n",
        "\n",
        "# You might also want to ensure the standard 'punkt' is downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print(\"'punkt' resource is already downloaded.\")\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK 'punkt' resource...\")\n",
        "    try:\n",
        "        nltk.download('punkt')\n",
        "        print(\"'punkt' resource download complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading 'punkt' resource: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while checking for 'punkt': {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cg2kvG50Bl4",
        "outputId": "fd211a75-d893-499c-be31-ca9590e771bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK 'punkt_tab' resource...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'punkt_tab' resource download complete.\n",
            "'punkt' resource is already downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}